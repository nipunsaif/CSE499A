{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13413962,"sourceType":"datasetVersion","datasetId":8513368},{"sourceId":13441278,"sourceType":"datasetVersion","datasetId":8531534},{"sourceId":13441573,"sourceType":"datasetVersion","datasetId":8531755}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##  Environment and Path Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport pickle\nimport cv2\nimport numpy as np\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\n\n# --- Path Configuration ---\n# The base path now includes the 'JAAD' subfolder.\nBASE_INPUT_PATH = '/kaggle/input/jaad-with-annotations/JAAD'\nOUTPUT_DATA_PATH = '/kaggle/working/jaad_output'\n\n# Create the output directory if it doesn't exist\nif not os.path.exists(OUTPUT_DATA_PATH):\n    print(f\"Creating output directory at: {OUTPUT_DATA_PATH}\")\n    os.makedirs(OUTPUT_DATA_PATH)\n\nprint(\"Paths configured:\")\nprint(f\"Base Input Path: {BASE_INPUT_PATH}\")\nprint(f\"Output Path: {OUTPUT_DATA_PATH}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:34:16.399852Z","iopub.execute_input":"2025-10-20T13:34:16.400037Z","iopub.status.idle":"2025-10-20T13:34:19.386582Z","shell.execute_reply.started":"2025-10-20T13:34:16.400020Z","shell.execute_reply":"2025-10-20T13:34:19.385810Z"}},"outputs":[{"name":"stdout","text":"Creating output directory at: /kaggle/working/jaad_output\nPaths configured:\nBase Input Path: /kaggle/input/jaad-with-annotations/JAAD\nOutput Path: /kaggle/working/jaad_output\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## File Mapping","metadata":{}},{"cell_type":"code","source":"# --- Define sub-directory paths (UPDATED for multi-level annotations) ---\nVIDEOS_PATH = os.path.join(BASE_INPUT_PATH, 'JAAD clips')\n\n# Create a dictionary to hold the paths for all annotation types\nANNOTATION_PATHS = {\n    'primary':    os.path.join(BASE_INPUT_PATH, 'JAAD annotations', 'annotations'),\n    'appearance': os.path.join(BASE_INPUT_PATH, 'JAAD annotations', 'annotations_appearance'),\n    'attributes': os.path.join(BASE_INPUT_PATH, 'JAAD annotations', 'annotations_attributes'),\n    'traffic':    os.path.join(BASE_INPUT_PATH, 'JAAD annotations', 'annotations_traffic'),\n    'vehicle':    os.path.join(BASE_INPUT_PATH, 'JAAD annotations', 'annotations_vehicle'),\n}\n\nprint(\"--- Paths Configuration ---\")\nprint(f\"Videos Path: {VIDEOS_PATH}\")\nfor key, path in ANNOTATION_PATHS.items():\n    print(f\"Annotations ({key}): {path}\")\n\n# --- List all video and annotation files ---\nvideo_files = sorted([f for f in os.listdir(VIDEOS_PATH) if f.endswith('.mp4')])\nprint(f\"\\nFound {len(video_files)} video files.\")\n\n# Create a dictionary of file lists for faster lookups.\nannotation_file_lists = {}\nfor key, path in ANNOTATION_PATHS.items():\n    if os.path.exists(path):\n        annotation_file_lists[key] = set(os.listdir(path))\n        print(f\"Found {len(annotation_file_lists[key])} '{key}' annotation files.\")\n    else:\n        print(f\"Warning: Directory not found for '{key}' annotations: {path}\")\n        annotation_file_lists[key] = set()\n\n# --- Create the multi-level mapping ---\nfile_mapping = {}\n\n# Iterate through the video files\nfor video_filename in video_files:\n    base_name = os.path.splitext(video_filename)[0]\n    current_video_map = {\n        'video_path': os.path.join(VIDEOS_PATH, video_filename),\n        'annotations': {}\n    }\n    \n    primary_annot_file = f\"{base_name}.xml\"\n    if 'primary' in annotation_file_lists and primary_annot_file in annotation_file_lists['primary']:\n        current_video_map['annotations']['primary'] = os.path.join(ANNOTATION_PATHS['primary'], primary_annot_file)\n\n    for key in ['appearance', 'attributes', 'traffic', 'vehicle']:\n        expected_annot_file = f\"{base_name}_{key}.xml\"\n        if key in annotation_file_lists and expected_annot_file in annotation_file_lists[key]:\n            current_video_map['annotations'][key] = os.path.join(ANNOTATION_PATHS[key], expected_annot_file)\n            \n    file_mapping[base_name] = current_video_map\n\nprint(f\"\\nSuccessfully created a mapping for {len(file_mapping)} video clips.\")\nprint(\"\\n--- Example of Mapped Files ---\")\nexample_key = list(file_mapping.keys())[0]\nprint(f\"ID: {example_key}\")\nprint(f\"  -> Video Path: {file_mapping[example_key]['video_path']}\")\nfor key, path in file_mapping[example_key]['annotations'].items():\n    print(f\"    - {key}: {path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:34:24.302843Z","iopub.execute_input":"2025-10-20T13:34:24.303507Z","iopub.status.idle":"2025-10-20T13:34:24.501883Z","shell.execute_reply.started":"2025-10-20T13:34:24.303480Z","shell.execute_reply":"2025-10-20T13:34:24.501173Z"}},"outputs":[{"name":"stdout","text":"--- Paths Configuration ---\nVideos Path: /kaggle/input/jaad-with-annotations/JAAD/JAAD clips\nAnnotations (primary): /kaggle/input/jaad-with-annotations/JAAD/JAAD annotations/annotations\nAnnotations (appearance): /kaggle/input/jaad-with-annotations/JAAD/JAAD annotations/annotations_appearance\nAnnotations (attributes): /kaggle/input/jaad-with-annotations/JAAD/JAAD annotations/annotations_attributes\nAnnotations (traffic): /kaggle/input/jaad-with-annotations/JAAD/JAAD annotations/annotations_traffic\nAnnotations (vehicle): /kaggle/input/jaad-with-annotations/JAAD/JAAD annotations/annotations_vehicle\n\nFound 346 video files.\nFound 346 'primary' annotation files.\nFound 346 'appearance' annotation files.\nFound 346 'attributes' annotation files.\nFound 346 'traffic' annotation files.\nFound 346 'vehicle' annotation files.\n\nSuccessfully created a mapping for 346 video clips.\n\n--- Example of Mapped Files ---\nID: video_0001\n  -> Video Path: /kaggle/input/jaad-with-annotations/JAAD/JAAD clips/video_0001.mp4\n    - primary: /kaggle/input/jaad-with-annotations/JAAD/JAAD annotations/annotations/video_0001.xml\n    - appearance: /kaggle/input/jaad-with-annotations/JAAD/JAAD annotations/annotations_appearance/video_0001_appearance.xml\n    - attributes: /kaggle/input/jaad-with-annotations/JAAD/JAAD annotations/annotations_attributes/video_0001_attributes.xml\n    - traffic: /kaggle/input/jaad-with-annotations/JAAD/JAAD annotations/annotations_traffic/video_0001_traffic.xml\n    - vehicle: /kaggle/input/jaad-with-annotations/JAAD/JAAD annotations/annotations_vehicle/video_0001_vehicle.xml\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Data Parsing","metadata":{}},{"cell_type":"code","source":"def parse_jaad_xml(xml_path):\n    \"\"\"\n    Parses a JAAD primary annotation XML file to extract pedestrian data.\n    \n    Args:\n        xml_path (str): The file path to the XML annotation.\n\n    Returns:\n        dict: A dictionary where keys are pedestrian IDs and values are another\n              dictionary mapping frame numbers to their attributes (bbox, behavior).\n    \"\"\"\n    if not os.path.exists(xml_path):\n        print(f\"Warning: XML path does not exist: {xml_path}\")\n        return {}\n        \n    try:\n        tree = ET.parse(xml_path)\n    except ET.ParseError:\n        print(f\"Warning: Could not parse XML file: {xml_path}\")\n        return {}\n        \n    root = tree.getroot()\n    \n    pedestrians_data = {}\n    \n    # Find all 'track' elements with label 'pedestrian'\n    for track in root.findall(\".//track[@label='pedestrian']\"):\n        ped_id = track.get('id')\n        pedestrians_data[ped_id] = {}\n        \n        # Iterate over each frame's bounding box for this pedestrian\n        for box in track.findall('box'):\n            frame_num = int(box.get('frame'))\n            \n            # Bounding box coordinates\n            bbox = {\n                'xtl': float(box.get('xtl')),\n                'ytl': float(box.get('ytl')),\n                'xbr': float(box.get('xbr')),\n                'ybr': float(box.get('ybr')),\n            }\n            \n            # --- FIX STARTS HERE ---\n            behavior = {}\n            # First, find the parent 'behavior' attribute element\n            behavior_element = box.find('attribute[@name=\"behavior\"]')\n            \n            # CRITICAL: Check if this element exists before trying to iterate its children\n            if behavior_element is not None:\n                # If it exists, now we can safely loop through its children\n                for beh in behavior_element:\n                    behavior[beh.get('name')] = beh.text\n            # --- FIX ENDS HERE ---\n\n            pedestrians_data[ped_id][frame_num] = {\n                'bbox': bbox,\n                'behavior': behavior # This will be an empty dict {} if no behavior tag was found\n            }\n            \n    return pedestrians_data\n\n# --- Example of parsing a single file ---\nexample_video_id = 'video_0001'\n# Ensure the key exists before accessing it\nif example_video_id in file_mapping and 'primary' in file_mapping[example_video_id]['annotations']:\n    example_xml_path = file_mapping[example_video_id]['annotations']['primary']\n    parsed_data_example = parse_jaad_xml(example_xml_path)\n\n    # Print a snippet of the parsed data for one pedestrian\n    if parsed_data_example:\n        example_ped_id = list(parsed_data_example.keys())[0]\n        print(f\"--- Parsed Data Example for Video '{example_video_id}', Pedestrian '{example_ped_id}' ---\")\n        # Check if frames exist for the pedestrian\n        if parsed_data_example[example_ped_id]:\n            example_frame_1 = parsed_data_example[example_ped_id].get(1, \"No data for frame 1\")\n            example_frame_2 = parsed_data_example[example_ped_id].get(2, \"No data for frame 2\")\n            print(f\"Frame 1 data: {example_frame_1}\")\n            print(f\"Frame 2 data: {example_frame_2}\")\n        else:\n            print(f\"No frames found for pedestrian {example_ped_id}\")\n    else:\n        print(f\"No pedestrian data parsed for video {example_video_id}\")\nelse:\n    print(f\"Primary annotation for '{example_video_id}' not found in file mapping.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:34:30.151001Z","iopub.execute_input":"2025-10-20T13:34:30.151271Z","iopub.status.idle":"2025-10-20T13:34:30.187601Z","shell.execute_reply.started":"2025-10-20T13:34:30.151250Z","shell.execute_reply":"2025-10-20T13:34:30.186861Z"}},"outputs":[{"name":"stdout","text":"--- Parsed Data Example for Video 'video_0001', Pedestrian 'None' ---\nFrame 1 data: {'bbox': {'xtl': 1402.0, 'ytl': 655.0, 'xbr': 1490.0, 'ybr': 894.0}, 'behavior': {}}\nFrame 2 data: {'bbox': {'xtl': 1406.0, 'ytl': 656.0, 'xbr': 1493.0, 'ybr': 897.0}, 'behavior': {}}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Data Acquisition and Splitting","metadata":{}},{"cell_type":"code","source":"all_videos_data = {}\n\nprint(\"Parsing annotations for all videos...\")\n# Using tqdm for a progress bar\nfor video_id, paths in tqdm(file_mapping.items(), desc=\"Processing Videos\"):\n    if 'primary' in paths['annotations']:\n        xml_path = paths['annotations']['primary']\n        all_videos_data[video_id] = parse_jaad_xml(xml_path)\n\nprint(f\"\\nSuccessfully parsed data for {len(all_videos_data)} videos.\")\n\n# --- Split video IDs into train, validation, and test sets ---\nvideo_ids = list(all_videos_data.keys())\ntrain_val_ids, test_ids = train_test_split(video_ids, test_size=0.15, random_state=42)\ntrain_ids, val_ids = train_test_split(train_val_ids, test_size=0.15, random_state=42) # 0.15 * 0.85 = ~13%\n\nprint(f\"\\nDataset Split:\")\nprint(f\"Total Videos: {len(video_ids)}\")\nprint(f\"Training set: {len(train_ids)} videos\")\nprint(f\"Validation set: {len(val_ids)} videos\")\nprint(f\"Test set: {len(test_ids)} videos\")\n\n# Store the splits for later use\ndataset_splits = {\n    'train': train_ids,\n    'val': val_ids,\n    'test': test_ids\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:34:34.968450Z","iopub.execute_input":"2025-10-20T13:34:34.968724Z","iopub.status.idle":"2025-10-20T13:34:46.012172Z","shell.execute_reply.started":"2025-10-20T13:34:34.968704Z","shell.execute_reply":"2025-10-20T13:34:46.011390Z"}},"outputs":[{"name":"stdout","text":"Parsing annotations for all videos...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Videos:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00f66e102daa409d87bd90b62973ec64"}},"metadata":{}},{"name":"stdout","text":"\nSuccessfully parsed data for 346 videos.\n\nDataset Split:\nTotal Videos: 346\nTraining set: 249 videos\nValidation set: 45 videos\nTest set: 52 videos\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install -q ultralytics torch torchvision numpy opencv-python tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:40:32.155534Z","iopub.execute_input":"2025-10-20T13:40:32.156152Z","iopub.status.idle":"2025-10-20T13:41:49.047022Z","shell.execute_reply.started":"2025-10-20T13:40:32.156130Z","shell.execute_reply":"2025-10-20T13:41:49.046288Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntransformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport cv2\nfrom ultralytics import YOLO\nfrom tqdm.notebook import tqdm\n\n# --- 1. Motion Activity: Transformer Architecture ---\nclass PoseTransformer(nn.Module):\n    def __init__(self, num_keypoints=17, embed_dim=64, nhead=4, num_layers=2, num_classes=4):\n        super().__init__()\n        self.input_projection = nn.Linear(num_keypoints * 2, embed_dim)\n        self.pos_encoder = nn.Parameter(torch.randn(1, 50, embed_dim))\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, dim_feedforward=128, batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.classifier = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        B, T, F = x.shape\n        x = self.input_projection(x) + self.pos_encoder[:, :T, :]\n        x = self.transformer(x)\n        output = self.classifier(x[:, -1, :])\n        return output\n\n# --- Helper for Loading Models ---\ndef load_models():\n    pose_model = YOLO('yolov8n-pose.pt')\n    motion_model = PoseTransformer()\n    motion_model.eval()\n    return pose_model, motion_model\n\n# --- Main Feature Extractor Class with FIX ---\nclass JAADFeatureExtractor:\n    def __init__(self):\n        print(\"Loading extraction models...\")\n        # Removed seg_model as it wasn't used in the final implementation\n        self.pose_model, self.motion_model = load_models()\n        print(\"Models loaded.\")\n        self.focal_length_px = 1000\n        self.avg_human_height_m = 1.7\n\n    def get_pose_keypoints(self, img, bbox):\n        x1, y1, x2, y2 = map(int, [bbox['xtl'], bbox['ytl'], bbox['xbr'], bbox['ybr']])\n        h_img, w_img = img.shape[:2]\n        x1, y1 = max(0, x1), max(0, y1)\n        x2, y2 = min(w_img, x2), min(h_img, y2)\n        if x2 - x1 < 10 or y2 - y1 < 10: return None\n        crop = img[y1:y2, x1:x2]\n        results = self.pose_model(crop, verbose=False)\n        if results[0].keypoints is not None and results[0].keypoints.xy.shape[0] > 0:\n            kpts = results[0].keypoints.xy[0].cpu().numpy()\n            kpts[:, 0] += x1; kpts[:, 1] += y1\n            return kpts\n        return None\n\n    def extract_motion_activity(self, pose_sequence):\n        if len(pose_sequence) < 2: return \"undefined (insufficient frames)\"\n        seq_tensor = torch.tensor(np.array(pose_sequence)).float().flatten(start_dim=1).unsqueeze(0)\n        with torch.no_grad():\n            logits = self.motion_model(seq_tensor)\n            action_idx = torch.argmax(logits, dim=1).item()\n        actions = {0: 'standing', 1: 'walking', 2: 'starting_to_cross', 3: 'crossing'}\n        return actions.get(action_idx, 'unknown')\n\n    # --- CORRECTED METHOD SIGNATURE ---\n    def extract_proximity_to_road(self, img_shape, bbox):\n        h, w = img_shape[:2]\n        road_mask = np.zeros((h, w), dtype=np.uint8); road_mask[int(h*0.7):, :] = 1\n        ped_x, ped_y = int((bbox['xtl'] + bbox['xbr']) / 2), int(bbox['ybr'])\n        ped_y, ped_x = min(ped_y, h - 1), min(ped_x, w - 1)\n        if road_mask[ped_y, ped_x] > 0: return 0.0, True\n        dist_matrix = cv2.distanceTransform(1 - road_mask, cv2.DIST_L2, 5)\n        distance_px = dist_matrix[ped_y, ped_x]\n        return float(distance_px), bool(distance_px < 50)\n\n    def extract_distance(self, bbox):\n        h_img = bbox['ybr'] - bbox['ytl']\n        if h_img <= 0: return None\n        return (self.focal_length_px * self.avg_human_height_m) / h_img\n\n    def extract_orientation(self, kpts):\n        if kpts is None or np.all(kpts[5] == 0) or np.all(kpts[6] == 0): return None\n        shoulder_vec = kpts[5] - kpts[6]\n        return np.degrees(np.arctan2(shoulder_vec[1], shoulder_vec[0]))\n\n    def extract_gaze(self, kpts):\n        if kpts is None or np.all(kpts[0] == 0) or (np.all(kpts[1] == 0) and np.all(kpts[2] == 0)): return None\n        eye_mid = (kpts[1] + kpts[2]) / 2.0\n        gaze_vec = kpts[0] - eye_mid\n        gaze_angle = np.degrees(np.arctan2(gaze_vec[1], gaze_vec[0]))\n        return {'vector': gaze_vec.tolist(), 'angle_2d': gaze_angle}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:53:40.210463Z","iopub.execute_input":"2025-10-20T13:53:40.211220Z","iopub.status.idle":"2025-10-20T13:53:40.226479Z","shell.execute_reply.started":"2025-10-20T13:53:40.211185Z","shell.execute_reply":"2025-10-20T13:53:40.225776Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Initialize the extractor once. This will load the new class definition from Cell 1.\nextractor = JAADFeatureExtractor()\n\n# This dictionary will store all results\nall_videos_features = {}\n\n# Main loop to iterate over every video in the dataset\nfor vid_id in tqdm(all_videos_data.keys(), desc=\"Processing All Videos\"):\n    video_path = file_mapping[vid_id]['video_path']\n    ped_data_for_video = all_videos_data[vid_id]\n    \n    if not ped_data_for_video:\n        continue\n\n    features_for_current_video = {}\n    cap = cv2.VideoCapture(video_path)\n    \n    # Inner loop for every pedestrian in the current video\n    for ped_id, frames_data in ped_data_for_video.items():\n        pedestrian_features = {}\n        pose_history = []\n        \n        for frame_num in sorted(frames_data.keys()):\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num - 1)\n            ret, frame = cap.read()\n            if not ret: break\n            \n            bbox = frames_data[frame_num]['bbox']\n            kpts = extractor.get_pose_keypoints(frame, bbox)\n            \n            if kpts is not None:\n                pose_history.append(kpts)\n            if len(pose_history) > 16:\n                pose_history.pop(0)\n\n            # --- CORRECTED AND EFFICIENT CALL ---\n            # Call the function once and store its two return values\n            prox_dist, is_near = extractor.extract_proximity_to_road(frame.shape, bbox)\n\n            pedestrian_features[frame_num] = {\n                'F1_Motion': extractor.extract_motion_activity(pose_history),\n                'F2_Proximity_px': prox_dist,\n                'F2_IsNearRoad': is_near,\n                'F3_Distance_m': extractor.extract_distance(bbox),\n                'F4_Orientation_deg': extractor.extract_orientation(kpts),\n                'F5_Gaze': extractor.extract_gaze(kpts)\n            }\n            \n        features_for_current_video[ped_id] = pedestrian_features\n\n    cap.release()\n    all_videos_features[vid_id] = features_for_current_video\n\nprint(\"\\n--- Full Dataset Extraction Complete ---\")\nprint(f\"Successfully processed {len(all_videos_features)} videos.\")\n\n# Display a sample of the extracted data\nif all_videos_features:\n    sample_vid_id = list(all_videos_features.keys())[0]\n    if all_videos_features[sample_vid_id]:\n        sample_ped_id = list(all_videos_features[sample_vid_id].keys())[0]\n        if all_videos_features[sample_vid_id][sample_ped_id]:\n            sample_frame_num = list(all_videos_features[sample_vid_id][sample_ped_id].keys())[0]\n            print(\"\\n--- Example of Extracted Data Structure ---\")\n            print(f\"Video ID: {sample_vid_id}\")\n            print(f\"  -> Pedestrian ID: {sample_ped_id}\")\n            print(f\"    -> Frame Number: {sample_frame_num}\")\n            print(f\"      -> Features: {all_videos_features[sample_vid_id][sample_ped_id][sample_frame_num]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T13:53:54.815292Z","iopub.execute_input":"2025-10-20T13:53:54.815890Z","iopub.status.idle":"2025-10-20T22:02:10.570966Z","shell.execute_reply.started":"2025-10-20T13:53:54.815862Z","shell.execute_reply":"2025-10-20T22:02:10.569922Z"}},"outputs":[{"name":"stdout","text":"Loading extraction models...\nModels loaded.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing All Videos:   0%|          | 0/346 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"276fdf3e52b349ac86e5b4985ad837af"}},"metadata":{}},{"name":"stdout","text":"\n--- Full Dataset Extraction Complete ---\nSuccessfully processed 320 videos.\n\n--- Example of Extracted Data Structure ---\nVideo ID: video_0001\n  -> Pedestrian ID: None\n    -> Frame Number: 0\n      -> Features: {'F1_Motion': 'undefined (insufficient frames)', 'F2_Proximity_px': 0.0, 'F2_IsNearRoad': True, 'F3_Distance_m': 7.142857142857143, 'F4_Orientation_deg': -0.99080575, 'F5_Gaze': {'vector': [1.058837890625, 3.96221923828125], 'angle_2d': 75.03826}}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import pickle\nimport json\nimport os\n\n# Define the output path in the Kaggle working directory\noutput_dir = \"/kaggle/working/\"\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# --- Option 1: Save as a Pickle file (Recommended for Python) ---\n# Pickle is fast, efficient, and preserves Python data types perfectly.\n# It's the ideal choice if you plan to load this data back into another Python script.\ntry:\n    pickle_path = os.path.join(output_dir, \"jaad_extracted_features.pkl\")\n    print(f\"\\n--- Saving data to Pickle file ---\")\n    print(f\"File path: {pickle_path}\")\n\n    with open(pickle_path, 'wb') as f:\n        # 'wb' stands for 'write binary', which is required for pickle\n        pickle.dump(all_videos_features, f)\n\n    print(\"✅ Successfully saved features to Pickle file.\")\n    file_size_mb = os.path.getsize(pickle_path) / (1024 * 1024)\n    print(f\"File size: {file_size_mb:.2f} MB\")\n\nexcept Exception as e:\n    print(f\"❌ Error saving to Pickle: {e}\")\n\n\n# --- Option 2: Save as a JSON file (For readability or use with other languages) ---\n# JSON is a human-readable text format. It's great for inspecting the data manually\n# or sharing it with applications written in other languages (e.g., JavaScript).\n# Note: This can be much slower and result in a larger file than pickle for large datasets.\n\n# Uncomment the following lines to also save as JSON\n# try:\n#     json_path = os.path.join(output_dir, \"jaad_extracted_features.json\")\n#     print(f\"\\n--- Saving data to JSON file ---\")\n#     print(f\"File path: {json_path}\")\n#\n#     with open(json_path, 'w') as f:\n#         # 'w' stands for 'write text'\n#         # indent=4 makes the file readable but increases its size\n#         json.dump(all_videos_features, f, indent=4)\n#\n#     print(\"✅ Successfully saved features to JSON file.\")\n#     file_size_mb = os.path.getsize(json_path) / (1024 * 1024)\n#     print(f\"File size: {file_size_mb:.2f} MB\")\n#\n# except Exception as e:\n#     print(f\"❌ Error saving to JSON: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T22:02:30.714746Z","iopub.execute_input":"2025-10-20T22:02:30.714996Z","iopub.status.idle":"2025-10-20T22:02:30.965468Z","shell.execute_reply.started":"2025-10-20T22:02:30.714979Z","shell.execute_reply":"2025-10-20T22:02:30.964629Z"}},"outputs":[{"name":"stdout","text":"\n--- Saving data to Pickle file ---\nFile path: /kaggle/working/jaad_extracted_features.pkl\n✅ Successfully saved features to Pickle file.\nFile size: 4.91 MB\n","output_type":"stream"}],"execution_count":18}]}